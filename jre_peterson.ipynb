{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tag list\n",
    "\n",
    "**CC**\tcoordinating conjunction<br>\n",
    "**CD**\tcardinal digit<br>\n",
    "**DT**\tdeterminer<br>\n",
    "**EX**\texistential there (like: \"there is\" ... think of it like \"there exists\")<br>\n",
    "**FW**\tforeign word<br>\n",
    "**IN**\tpreposition/subordinating conjunction<br>\n",
    "**JJ**\tadjective\t'big'<br>\n",
    "**JJR**\tadjective, comparative\t'bigger'<br>\n",
    "**JJS**\tadjective, superlative\t'biggest'<br>\n",
    "**LS**\tlist marker\t1)<br>\n",
    "**MD**\tmodal\tcould, will<br>\n",
    "**NN**\tnoun, singular 'desk'<br>\n",
    "**NNS**\tnoun plural\t'desks'<br>\n",
    "**NNP**\tproper noun, singular\t'Harrison'<br>\n",
    "**NNPS**\tproper noun, plural\t'Americans'<br>\n",
    "**PDT**\tpredeterminer\t'all the kids'<br>\n",
    "**POS**\tpossessive ending\tparent's<br>\n",
    "**PRP**\tpersonal pronoun\tI, he, she<br>\n",
    "**PRP\\$**\tpossessive pronoun\tmy, his, hers<br>\n",
    "**RB**\tadverb\tvery, silently,<br>\n",
    "**RBR**\tadverb, comparative\tbetter<br>\n",
    "**RBS**\tadverb, superlative\tbest<br>\n",
    "**RP**\tparticle\tgive up<br>\n",
    "**TO**\tto\tgo 'to' the store.<br>\n",
    "**UH**\tinterjection\terrrrrrrrm<br>\n",
    "**VB**\tverb, base form\ttake<br>\n",
    "**VBD**\tverb, past tense\ttook<br>\n",
    "**VBG**\tverb, gerund/present participle\ttaking<br>\n",
    "**VBN**\tverb, past participle\ttaken<br>\n",
    "**VBP**\tverb, sing. present, non-3d\ttake<br>\n",
    "**VBZ**\tverb, 3rd person sing. present\ttakes<br>\n",
    "**WDT**\twh-determiner\twhich<br>\n",
    "**WP**\twh-pronoun\twho, what<br>\n",
    "**WP\\$**\tpossessive wh-pronoun\twhose<br>\n",
    "**WRB**\twh-abverb\twhere, when<br>\n",
    "\n",
    "### Stopwords\n",
    "\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",<br>\n",
    "\"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', <br>\n",
    "'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\",<br>\n",
    "'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', <br>\n",
    "'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', <br>\n",
    "'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', <br>\n",
    "'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',<br>\n",
    "'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',<br>\n",
    "'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',<br>\n",
    "'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',<br>\n",
    "'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',<br>\n",
    "'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',<br>\n",
    "'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',<br>\n",
    "'s', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now',<br>\n",
    "'d', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",<br>\n",
    "'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', <br>\n",
    "\"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn',<br>\n",
    "\"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', <br>\n",
    "\"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corpus(url):\n",
    "    '''Pass in url. Retrieve content and return list of paragraphs.'''\n",
    "    content = requests.get(url).text\n",
    "    soup = BS(content, 'lxml')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    paragraphs = [p.get_text() for p in paragraphs]\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def view_tails(paragraphs, start=10, end=10):\n",
    "    '''Pass in list of paragraphs and print out first 5 and last 5.\n",
    "    Use to quickly check what needs to be trimmed.\n",
    "    '''\n",
    "    pprint(paragraphs[:start])\n",
    "    print('\\n')\n",
    "    pprint(paragraphs[-end:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_curly_quotes(*corpora):\n",
    "    '''Pass in 1 or more corpora and replace all curly quotes\n",
    "    with standard quotes.\n",
    "    '''\n",
    "    output = []\n",
    "    for i, corpus in enumerate(corpora):\n",
    "        new_corpus = corpus.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "        output.append(new_corpus)\n",
    "    return output\n",
    "\n",
    "def replace_curly_str(paragraph):\n",
    "    '''Pass in string and replace all curly quotes\n",
    "    with standard quotes.\n",
    "    '''\n",
    "    output = paragraph.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_corpus(paragraphs, trim_start=0, trim_end=0, delete=None, replace_quotes=False, *args):\n",
    "    '''Pass in list of paragraphs, # of paragraphs to drop from beginning\n",
    "    and end, filler paragraphs to delete, bool specifying whether to replace \n",
    "    curly quotes, and optional tuples of format (to_replace, replace_with).\n",
    "    '''\n",
    "    # Trim list first before any paragraphs are deleted, changing the length.\n",
    "    trim_end = len(paragraphs) - trim_end\n",
    "    paragraphs = paragraphs[trim_start:trim_end]\n",
    "    \n",
    "    # Replace curly quotes and delete filler paragraphs. Check which params\n",
    "    # were passed in so we only have to iterate over the list once here.\n",
    "    if replace_quotes and delete is not None:\n",
    "        paragraphs = [replace_curly_str(p) for p in paragraphs if p != delete]\n",
    "    elif replace_quotes:\n",
    "        paragraphs = [replace_curly_str(p) for p in paragraphs]\n",
    "    elif delete is not None:\n",
    "        paragraphs = [p for p in paragraphs if p != delete]\n",
    "    \n",
    "    # Make any miscellaneous replacements specified by user.\n",
    "    for arg in args:\n",
    "        paragraphs = [p.replace(arg[0], arg[1]) for p in paragraphs]\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def separate_speakers(paragraphs, speakers, suffix=':'):\n",
    "#     '''Pass in list of paragraphs and list of 2 speaker names. Return dict\n",
    "#     with list of paragraphs for each speaker.\n",
    "#     '''\n",
    "#     speaker_lists = {}\n",
    "#     speaker_suffs = [s + suffix for s in speakers]\n",
    "#     speaker_lower = [s.lower() for s in speakers]\n",
    "#     for s_suff, s_low in zip(speaker_suffs, speaker_lower):\n",
    "#         speaker_lists[s_low] = [p.replace(s_suff, '').strip() for p in paragraphs\\\n",
    "#                                 if p.startswith(s_suff)]\n",
    "#     speaker_lists['unlabeled'] = [p for p in paragraphs if not\\\n",
    "#                                   any(p.startswith(s) for s in speaker_suffs)]\n",
    "#     #stripped_paragraphs = [p.strip(s_suff).strip() for p in paragraphs for s_suff in speaker_suffs]\n",
    "#     #speaker_lists['unlabeled'] = list(set(stripped_paragraphs) - \\\n",
    "#     #    (set(speaker_lists[speaker_lower[0]]) | set(speaker_lists[speaker_lower[1]])))\n",
    "#     return speaker_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_speakers(paragraphs, speakers, suffix=':'):\n",
    "    '''Pass in list of paragraphs and list of 2 speaker names. Return dict\n",
    "    with list of paragraphs for each speaker.\n",
    "    '''\n",
    "    speaker_suffs = [s + suffix for s in speakers]\n",
    "    speaker_lower = [s.lower() for s in speakers]\n",
    "    speaker_lists = {s: [] for s in speaker_lower}\n",
    "    speaker_lists['unlabeled'] = []\n",
    "    for p in paragraphs:\n",
    "        assigned = False\n",
    "        for s_suff, s_low in zip(speaker_suffs, speaker_lower):\n",
    "            if p.startswith(s_suff):\n",
    "                speaker_lists[s_low].append(p.replace(s_suff, '').strip())\n",
    "                assigned = True\n",
    "        if not assigned:\n",
    "            speaker_lists['unlabeled'].append(p)\n",
    "    return speaker_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_speakers2(paragraphs, speakers, suffix=':'):\n",
    "    '''Pass in list of paragraphs and list of 2 speaker names. Return dict\n",
    "    with list of paragraphs for each speaker.\n",
    "    \n",
    "    Testing ways to deal with unlabeled data - assign to previous speaker?\n",
    "    '''\n",
    "    speaker_suffs = [s + suffix for s in speakers]\n",
    "    speaker_lower = [s.lower() for s in speakers]\n",
    "    speaker_lists = {s: [] for s in speaker_lower}\n",
    "    speaker_lists['unlabeled'] = []\n",
    "    \n",
    "    # Loop through list of paragraphs and assign each to a speaker.\n",
    "    # If no speaker label exists, assign to most recent speaker and\n",
    "    # also store in 'unlabeled' for manual checking.\n",
    "    for p in paragraphs:\n",
    "        assigned = False\n",
    "        for s_suff, s_low in zip(speaker_suffs, speaker_lower):\n",
    "            if p.startswith(s_suff):\n",
    "                speaker_lists[s_low].append(p.replace(s_suff, '').strip())\n",
    "                assigned = True\n",
    "                previous_speaker = s_low\n",
    "        if not assigned:\n",
    "            speaker_lists[previous_speaker].append(p)\n",
    "            speaker_lists['unlabeled'].append(p)\n",
    "    return speaker_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# refactoring 1070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IN PROGRESS - REPLACING ABOVE CELL 1 WITH FUNCTION CALLS\n",
    "url_1070 = r'https://erikamentari.wordpress.com/2018/02/27/jre-1070-jordan-peterson-transcript/'\n",
    "raw_1070 = get_corpus(url_1070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611\n",
      "163379\n"
     ]
    }
   ],
   "source": [
    "# IN PROGRESS - REPLACING BELOW CELL 2 WITH FUNCTION CALLS\n",
    "cleaned_1070 = clean_corpus(raw_1070, 8, 10, None, True, ('Joe Rogan: ', 'Joe: '), \n",
    "                            ('Dr Jordan B Peterson: ', 'Jordan: '))\n",
    "joined_1070 = ' '.join(cleaned_1070)\n",
    "\n",
    "print(len(cleaned_1070))\n",
    "print(len(joined_1070))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph count (Joe): \n",
      "306\n",
      "\n",
      "Paragraph count (Jordan): \n",
      "304\n",
      "\n",
      "Paragraph count (Unlabeled): \n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IN PROGRESS...\n",
    "p_1070 = separate_speakers(cleaned_1070, ['Joe', 'Jordan'])\n",
    "p_1070['joe'].insert(2, p_1070['unlabeled'].pop(0))\n",
    "for key, val in p_1070.items():\n",
    "    print('Paragraph count ({}): \\n{}\\n'.format(key.title(), len(val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Episode 1070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 611\n",
      "Words: 163379\n"
     ]
    }
   ],
   "source": [
    "# REPLACING\n",
    "url = r'https://erikamentari.wordpress.com/2018/02/27/jre-1070-jordan-peterson-transcript/'\n",
    "\n",
    "content = requests.get(url).text\n",
    "soup = BS(content, 'lxml')\n",
    "paragraphs = soup.find_all('p')\n",
    "paragraphs = [p.get_text().replace('Joe Rogan: ', 'Joe: ')\\\n",
    "              .replace('Dr Jordan B Peterson: ', \"Jordan: \")\\\n",
    "              for p in paragraphs[8:-10]]\n",
    "paragraph_text = ' '.join(paragraphs)\n",
    "\n",
    "print('Paragraphs:', len(paragraphs))\n",
    "print('Words:', len(paragraph_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogan paragraph count: 306\n",
      "Peterson paragraph count: 304\n",
      "Unassigned text: ['18:18']\n"
     ]
    }
   ],
   "source": [
    "# REPLACING\n",
    "rogan_p = [p.replace('Joe:', '').strip() for p in paragraphs if p.startswith('Joe:')]\n",
    "peterson_p = [p.replace('Jordan:', '').strip() for p in paragraphs if p.startswith('Jordan:')]\n",
    "unlabeled = [p for p in paragraphs if not p.startswith('Joe:') and not p.startswith('Jordan:')]\n",
    "rogan_p.insert(1, unlabeled.pop(0))\n",
    "\n",
    "print('Rogan paragraph count:', len(rogan_p))\n",
    "print('Peterson paragraph count:', len(peterson_p))\n",
    "print('Unassigned text:', unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count: 29119\n",
      "Rogan word count: 6736\n",
      "Peterson word count: 22383\n",
      "% of conversation (Rogan): 23.13%\n",
      "% of conversation (Peterson): 76.87%\n"
     ]
    }
   ],
   "source": [
    "rogan_words = ' '.join(rogan_p).split(' ')\n",
    "peterson_words = ' '.join(peterson_p).split(' ')\n",
    "rogan_count, peterson_count = len(rogan_words), len(peterson_words)\n",
    "total_count = rogan_count + peterson_count\n",
    "rogan_percent = rogan_count / total_count\n",
    "peterson_percent = peterson_count / total_count\n",
    "\n",
    "print('Total word count:', total_count)\n",
    "print('Rogan word count:', rogan_count)\n",
    "print('Peterson word count:', peterson_count)\n",
    "print('% of conversation (Rogan): {}%'.format(round(100* rogan_percent, 2)))\n",
    "print('% of conversation (Peterson): {}%'.format(round(100 * peterson_percent, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (Rogan)\n",
      "\n",
      "[('to', 187), ('of', 182), ('the', 178), ('I', 170), ('and', 157), ('a', 155), ('that', 145), ('you', 127), ('is', 126), ('this', 91), ('in', 87), ('people', 65), ('And', 64), ('what', 59), ('have', 59), ('it', 56), ('about', 53), ('think', 51), ('was', 50), ('are', 47), ('you’re', 42), ('not', 42), ('it’s', 41), ('with', 38), ('one', 35)]\n",
      "\n",
      "Most common words (Peterson)\n",
      "\n",
      "[('the', 868), ('to', 614), ('and', 524), ('you', 517), ('of', 498), ('a', 493), ('that', 466), ('I', 430), ('is', 291), ('in', 262), ('And', 244), ('it', 219), ('it’s', 195), ('like,', 189), ('that’s', 181), ('It’s', 174), ('what', 168), ('they', 156), ('have', 149), ('so', 144), ('was', 135), ('for', 134), ('people', 133), ('be', 128), ('do', 125)]\n"
     ]
    }
   ],
   "source": [
    "common_rogan = Counter(rogan_words).most_common()\n",
    "common_peterson = Counter(peterson_words).most_common()\n",
    "\n",
    "print('Most common words (Rogan)\\n\\n' + str(common_rogan[:25]))\n",
    "print('\\nMost common words (Peterson)\\n\\n' + str(common_peterson[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rogan_corpus = ' '.join(rogan_words)\n",
    "peterson_corpus = ' '.join(peterson_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogan words: 8000\n",
      "Peterson words: 27448\n"
     ]
    }
   ],
   "source": [
    "# replace curly quotes, then tokenize\n",
    "rogan_corpus, peterson_corpus = replace_curly_quotes(rogan_corpus, peterson_corpus)\n",
    "rogan_word_toke, rogan_p_toke = word_tokenize(rogan_corpus), sent_tokenize(rogan_corpus)\n",
    "peterson_word_toke, peterson_p_toke = word_tokenize(peterson_corpus), sent_tokenize(peterson_corpus)\n",
    "\n",
    "print('Rogan words: ' + str(len(rogan_word_toke)))\n",
    "print('Peterson words: ' + str(len(peterson_word_toke)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogan words (stop words removed): 4205\n",
      "Peterson words (stop words removed): 15093\n",
      "\n",
      "Rogan most common words:\n",
      "[('.', 449), (',', 368), (\"'s\", 148), ('people', 75), (\"'re\", 73), ('?', 62), ('think', 53), ('like', 45), (\"n't\", 45), ('one', 35), ('things', 30), (';', 25), (\"'m\", 24), ('Yeah', 24), ('Right', 23), ('know', 23), ('Yes', 22), ('get', 22), ('going', 22), ('mean', 21), ('way', 20), (\"'ve\", 20), ('saying', 19), ('right', 17), ('Well', 17)]\n",
      "\n",
      "Peterson most common words\n",
      "[(',', 1853), ('.', 1348), (\"'s\", 816), ('like', 308), (\"n't\", 230), (\"'re\", 200), ('?', 191), ('well', 165), (';', 163), ('people', 157), ('know', 150), ('think', 120), ('Yeah', 99), ('Well', 97), ('going', 94), (\"'m\", 84), ('right', 81), ('say', 80), ('things', 70), ('way', 65), ('want', 65), ('one', 61), ('life', 58), ('thing', 58), ('get', 58)]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "rogan_word_toke = [w for w in rogan_word_toke if w.lower() not in stop_words]\n",
    "peterson_word_toke = [w for w in peterson_word_toke if w.lower() not in stop_words]\n",
    "\n",
    "print('Rogan words (stop words removed): ' + str(len(rogan_word_toke)))\n",
    "print('Peterson words (stop words removed): ' + str(len(peterson_word_toke)))\n",
    "print('\\nRogan most common words:\\n' + str(Counter(rogan_word_toke).most_common(25)))\n",
    "print('\\nPeterson most common words\\n' + str(Counter(peterson_word_toke).most_common(25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rogan_tagged = nltk.pos_tag(rogan_word_toke)\n",
    "peterson_tagged = nltk.pos_tag(rogan_word_toke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk(tagged):\n",
    "    '''Pass in tagged word tokens and find chunks.'''\n",
    "    chunk_gram = '''chunk_name: {<JJ.?>+<NN.?>+}'''\n",
    "    chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "    chunked = chunk_parser.parse(tagged)\n",
    "    #chunked.draw()\n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy_widget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +\n\u001b[0m\u001b[1;32m    731\u001b[0m                             \u001b[1;34m'-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                             .format(out_path, in_path).split())\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[1;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 604\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 598\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         (filename, url))\n\u001b[1;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n==========================================================================="
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('guest', 'NN'), ('today', 'NN'), Tree('chunk_name', [('great', 'JJ'), ('powerful', 'JJ'), ('Jordan', 'NNP'), ('Peterson', 'NNP')]), ('.', '.'), ('Jordan', 'NNP'), ('podcast', 'NN'), ('multiple', 'NN'), ('times', 'NNS'), (',', ','), (\"'s\", 'VBZ'), ('one', 'CD'), Tree('chunk_name', [('favorite', 'JJ'), ('human', 'JJ'), ('beings', 'NNS'), ('talk', 'NN')]), (',', ','), (\"'m\", 'VBP'), Tree('chunk_name', [('happy', 'JJ'), ('exists', 'NNS')]), ('.', '.'), (\"'s\", 'POS'), Tree('chunk_name', [('brilliant', 'JJ'), ('man', 'NN')]), ('amazing', 'VBG'), ('book', 'NN'), ('right', 'NN'), (\"'s\", 'POS'), ('called', 'VBN'), ('twelve', 'NN'), ('rules', 'NNS'), ('life', 'NN'), (':', ':'), Tree('chunk_name', [('antidote', 'JJ'), ('chaos', 'NN')]), ('one', 'CD'), Tree('chunk_name', [('important', 'JJ'), ('messages', 'NNS')]), Tree('chunk_name', [('brilliant', 'JJ'), ('man', 'NN')]), (',', ','), ('please', 'VB'), Tree('chunk_name', [('welcome', 'JJ'), ('Jordan', 'NNP'), ('Peterson', 'NNP')]), ('.', '.'), ('Boom', 'NNP'), (\"'re\", 'VBP'), ('live', 'JJ'), ('.', '.'), ('12', 'CD'), ('Rules', 'NNPS'), ('Life', 'NNP'), ('.', '.'), ('without', 'IN'), ('reading', 'VBG'), ('this…', 'NN'), (\"''\", \"''\"), (',', ','), (\"'re\", 'VBP'), ('saying', 'VBG'), ('is…', 'NN'), (\"''\", \"''\"), (',', ','), Tree('chunk_name', [('um—', 'JJ'), ('interview', 'NN'), ('woman', 'NN'), ('Cathy', 'NNP'), ('Newman', 'NNP')]), (',', ','), ('uh', 'UH'), (',', ','), ('UK', 'NNP'), ('?', '.'), ('Um', 'NNP'), (',', ','), ('went—', 'VBD'), ('felt', 'NN'), ('bad', 'JJ'), ('also', 'RB'), ('laughing—', 'VBP'), ('went', 'VBD'), ('Twitter', 'NNP'), ('page', 'NN'), ('read', 'VBD'), (',', ','), ('like', 'IN'), (',', ','), ('one', 'CD'), ('tweets', 'NNS'), (',', ','), ('matter', 'NN'), ('says', 'VBZ'), (';', ':'), ('someone', 'NN'), ('writes', 'VBZ'), ('underneath', 'IN'), (',', ',')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk(rogan_tagged[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import string\n",
    "\n",
    "# print(string.punctuation)\n",
    "# remove_punct = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "# a = 'abdlkja;c,3dk!.ejf}]dk'\n",
    "# print(a)\n",
    "# print(a.translate(remove_punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n",
      "389\n",
      "175\n",
      "174\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "url2 = ('https://beyondhumannature.wordpress.com/2018/03/12/joe-rogan-experience'\n",
    "        '-ep-877-with-jordan-peterson-transcript/')\n",
    "raw_p2 = get_corpus(url2)\n",
    "#view_tails(raw_p2, 1, 18)\n",
    "p2 = clean_corpus(raw_p2, 11, 16, '\\xa0', True)\n",
    "texts = separate_speakers(p2, ['ROGAN', 'PETERSON'])\n",
    "\n",
    "print(len(raw_p2))\n",
    "print(len(p2))\n",
    "print(len(texts['rogan']))\n",
    "print(len(texts['peterson']))\n",
    "print(len(texts['unlabeled']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 He's been battling this on his YouTube page, and the University sent him these letters asking him, n\n",
      "1 [MUSIC 172:38]\n",
      "2 But the initial idea was to eliminate the proclivity for people to be categorized according to their\n",
      "3 So for example, when the Soviets collectivized the farms, they pretty much wiped out, or raped and f\n",
      "4 But the Soviets were big on collective guilt, and all of these things that you hear about now, like \n",
      "5 And I think what happened is that the Marxist ideas are actually quite attractive if you're an intel\n",
      "6 And then those ideas were put into practice first in the Soviet Union, and Alexander Solzhenitsyn, w\n",
      "7 Now, what happened: There are always apologists for the left in the west, especially in France, espe\n",
      "8 The underlying pathological philosophy remained exactly the same, but the surface nomenclature chang\n",
      "9 And people have known historical memory. Like my students—and that's partly because they're taught s\n",
      "10 And people tend more than we ever expected—and I've done a lot of research, and this in my lab—that \n",
      "11 So, here's something to think about for everyone that thinks that equality of outcome is a good idea\n",
      "12 And it's appalling, because along with all this push for ethnic and sexual and racial diversity, whi\n",
      "13 The left describes anything that's associated with the assumption that someone who's female, for exa\n",
      "14 But the idea that you're going to get a diversity of ideas because you have a diversity of class of \n",
      "15 These people who are pushing equity, which is equality of outcome—that's what the word equity codes \n",
      "16 And sometimes you don't even know what don't you know about something. You know, I could say well, t\n",
      "17 It was very bizarre for me at that point in this, in the last few months, because I was under substa\n",
      "18 So everybody sacrifices a certain portion of their peculiarity to become a socialized creature, and \n",
      "19 Is it perfect? Well, obviously not. Does it require the sacrifice of individuality? Yes. Is some of \n",
      "20 You know, it's… If kids organize themselves in the playground to play hide-and-go-seek, they've marg\n",
      "21 And then you start to examine the laws, and you start to look at what's going on here, and this egal\n",
      "22 Personality psychologists are very, very careful about defining what they measure. And so, for examp\n",
      "23 And so, for example, if I showed you a bunch of pictures of black people, and a bunch of photographs\n",
      "24 apparently this agent that transforms cosmic energy into wisdom. I mean, she's completely…\n",
      "25 And if anybody's interested in this sort of process, and this is a horrifying book if you want to re\n",
      "26 And their commandant, their commander, was by all accounts a pretty decent guy, and he told them tha\n",
      "27 And so that's how things get to where they are now, is that—I mean I know they're not at the point, \n",
      "28 And so then the Soviet intellectuals went into the villages, and—just imagine how this happened— so \n",
      "29 So one of the things I try to do in my class, I have this class called \"Maps of Meaning\", which conc\n",
      "30 I'm a clinical psychologist and here's one of the things you do to make people less afraid: you don'\n",
      "31 And so then you say, \"Well, maybe you can go to a party for half an hour and all you have to do is i\n",
      "32 That's what you do at a university. You arm people with arguments, you hone their intellect. You hel\n",
      "33 And that's what we have in the universities. We have the reign of the \"Oedipal Mother\" whose answer \n",
      "34 And I think it's partly because we have all these radical-left, political-activist departments at th\n",
      "35 […] Just go online and look at a dozen Women's Studies websites. Just read them, you can see what th\n",
      "36 And so you can ask these… And what would they replace it with? They'd replace it with their own ideo\n",
      "37 You know, these kids who shoot up high schools, and these mass shooters, they're the perfect example\n",
      "38 transsexual.\n",
      "39 And the other thing that's really interesting about YouTube with regards to my, especially the more \n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(texts['unlabeled']):\n",
    "    print(i, text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
