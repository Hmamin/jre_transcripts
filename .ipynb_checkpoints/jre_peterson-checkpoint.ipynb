{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tag list\n",
    "\n",
    "**CC**\tcoordinating conjunction<br>\n",
    "**CD**\tcardinal digit<br>\n",
    "**DT**\tdeterminer<br>\n",
    "**EX**\texistential there (like: \"there is\" ... think of it like \"there exists\")<br>\n",
    "**FW**\tforeign word<br>\n",
    "**IN**\tpreposition/subordinating conjunction<br>\n",
    "**JJ**\tadjective\t'big'<br>\n",
    "**JJR**\tadjective, comparative\t'bigger'<br>\n",
    "**JJS**\tadjective, superlative\t'biggest'<br>\n",
    "**LS**\tlist marker\t1)<br>\n",
    "**MD**\tmodal\tcould, will<br>\n",
    "**NN**\tnoun, singular 'desk'<br>\n",
    "**NNS**\tnoun plural\t'desks'<br>\n",
    "**NNP**\tproper noun, singular\t'Harrison'<br>\n",
    "**NNPS**\tproper noun, plural\t'Americans'<br>\n",
    "**PDT**\tpredeterminer\t'all the kids'<br>\n",
    "**POS**\tpossessive ending\tparent's<br>\n",
    "**PRP**\tpersonal pronoun\tI, he, she<br>\n",
    "**PRP\\$**\tpossessive pronoun\tmy, his, hers<br>\n",
    "**RB**\tadverb\tvery, silently,<br>\n",
    "**RBR**\tadverb, comparative\tbetter<br>\n",
    "**RBS**\tadverb, superlative\tbest<br>\n",
    "**RP**\tparticle\tgive up<br>\n",
    "**TO**\tto\tgo 'to' the store.<br>\n",
    "**UH**\tinterjection\terrrrrrrrm<br>\n",
    "**VB**\tverb, base form\ttake<br>\n",
    "**VBD**\tverb, past tense\ttook<br>\n",
    "**VBG**\tverb, gerund/present participle\ttaking<br>\n",
    "**VBN**\tverb, past participle\ttaken<br>\n",
    "**VBP**\tverb, sing. present, non-3d\ttake<br>\n",
    "**VBZ**\tverb, 3rd person sing. present\ttakes<br>\n",
    "**WDT**\twh-determiner\twhich<br>\n",
    "**WP**\twh-pronoun\twho, what<br>\n",
    "**WP\\$**\tpossessive wh-pronoun\twhose<br>\n",
    "**WRB**\twh-abverb\twhere, when<br>\n",
    "\n",
    "### Stopwords\n",
    "\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",<br>\n",
    "\"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', <br>\n",
    "'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\",<br>\n",
    "'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', <br>\n",
    "'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', <br>\n",
    "'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', <br>\n",
    "'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',<br>\n",
    "'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',<br>\n",
    "'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',<br>\n",
    "'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',<br>\n",
    "'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',<br>\n",
    "'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',<br>\n",
    "'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',<br>\n",
    "'s', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now',<br>\n",
    "'d', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",<br>\n",
    "'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', <br>\n",
    "\"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn',<br>\n",
    "\"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', <br>\n",
    "\"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corpus(url):\n",
    "    '''Pass in url. Retrieve content and return list of paragraphs.'''\n",
    "    content = requests.get(url).text\n",
    "    soup = BS(content, 'lxml')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    paragraphs = [p.get_text() for p in paragraphs]\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def view_tails(paragraphs, start=10, end=10):\n",
    "    '''Pass in list of paragraphs and print out first 5 and last 5.\n",
    "    Use to quickly check what needs to be trimmed.\n",
    "    '''\n",
    "    pprint(paragraphs[:start])\n",
    "    print('\\n')\n",
    "    pprint(paragraphs[-end:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_curly_quotes(*corpora):\n",
    "    '''Pass in 1 or more corpora and replace all curly quotes\n",
    "    with standard quotes.\n",
    "    '''\n",
    "    output = []\n",
    "    for i, corpus in enumerate(corpora):\n",
    "        new_corpus = corpus.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "        output.append(new_corpus)\n",
    "    return output\n",
    "\n",
    "def replace_curly_str(paragraph):\n",
    "    '''Pass in string and replace all curly quotes\n",
    "    with standard quotes.\n",
    "    '''\n",
    "    output = paragraph.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_corpus(paragraphs, trim_start=0, trim_end=0, delete=None, replace_quotes=False, *args):\n",
    "    '''Pass in list of paragraphs, # of paragraphs to drop from beginning\n",
    "    and end, filler paragraphs to delete, bool specifying whether to replace \n",
    "    curly quotes, and optional tuples of format (to_replace, replace_with).\n",
    "    '''\n",
    "    # Trim list first before any paragraphs are deleted, changing the length.\n",
    "    trim_end = len(paragraphs) - trim_end\n",
    "    paragraphs = paragraphs[trim_start:trim_end]\n",
    "    \n",
    "    # Replace curly quotes and delete filler paragraphs. Check which params\n",
    "    # were passed in so we only have to iterate over the list once here.\n",
    "    if replace_quotes and delete is not None:\n",
    "        paragraphs = [replace_curly_str(p) for p in paragraphs if p != delete]\n",
    "    elif replace_quotes:\n",
    "        paragraphs = [replace_curly_str(p) for p in paragraphs]\n",
    "    elif delete is not None:\n",
    "        paragraphs = [p for p in paragraphs if p != delete]\n",
    "    \n",
    "    # Make any miscellaneous replacements specified by user.\n",
    "    for arg in args:\n",
    "        paragraphs = [p.replace(arg[0], arg[1]) for p in paragraphs]\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def separate_speakers(paragraphs, speakers, suffix=':'):\n",
    "#     '''Pass in list of paragraphs and list of 2 speaker names. Return dict\n",
    "#     with list of paragraphs for each speaker.\n",
    "#     '''\n",
    "#     speaker_lists = {}\n",
    "#     speaker_suffs = [s + suffix for s in speakers]\n",
    "#     speaker_lower = [s.lower() for s in speakers]\n",
    "#     for s_suff, s_low in zip(speaker_suffs, speaker_lower):\n",
    "#         speaker_lists[s_low] = [p.replace(s_suff, '').strip() for p in paragraphs\\\n",
    "#                                 if p.startswith(s_suff)]\n",
    "#     speaker_lists['unlabeled'] = [p for p in paragraphs if not\\\n",
    "#                                   any(p.startswith(s) for s in speaker_suffs)]\n",
    "#     #stripped_paragraphs = [p.strip(s_suff).strip() for p in paragraphs for s_suff in speaker_suffs]\n",
    "#     #speaker_lists['unlabeled'] = list(set(stripped_paragraphs) - \\\n",
    "#     #    (set(speaker_lists[speaker_lower[0]]) | set(speaker_lists[speaker_lower[1]])))\n",
    "#     return speaker_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def separate_speakers(paragraphs, speakers, suffix=':'):\n",
    "#     '''Pass in list of paragraphs and list of 2 speaker names. Return dict\n",
    "#     with list of paragraphs for each speaker.\n",
    "#     '''\n",
    "#     speaker_suffs = [s + suffix for s in speakers]\n",
    "#     speaker_lower = [s.lower() for s in speakers]\n",
    "#     speaker_lists = {s: [] for s in speaker_lower}\n",
    "#     speaker_lists['unlabeled'] = []\n",
    "#     for p in paragraphs:\n",
    "#         assigned = False\n",
    "#         for s_suff, s_low in zip(speaker_suffs, speaker_lower):\n",
    "#             if p.startswith(s_suff):\n",
    "#                 speaker_lists[s_low].append(p.replace(s_suff, '').strip())\n",
    "#                 assigned = True\n",
    "#         if not assigned:\n",
    "#             speaker_lists['unlabeled'].append(p)\n",
    "#     return speaker_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate_speakers(paragraphs, speakers, suffix=':'):\n",
    "    '''Pass in list of paragraphs and list of 2 speaker names. Return dict\n",
    "    with list of paragraphs for each speaker.\n",
    "    \n",
    "    Testing ways to deal with unlabeled data - assign to previous speaker?\n",
    "    '''\n",
    "    speaker_suffs = [s + suffix for s in speakers]\n",
    "    speaker_lower = [s.lower() for s in speakers]\n",
    "    speaker_lists = {s: [] for s in speaker_lower}\n",
    "    speaker_lists['unlabeled'] = []\n",
    "    \n",
    "    # Loop through list of paragraphs and assign each to a speaker.\n",
    "    # If no speaker label exists, assign to most recent speaker and\n",
    "    # also store in 'unlabeled' for manual checking.\n",
    "    for p in paragraphs:\n",
    "        assigned = False\n",
    "        for s_suff, s_low in zip(speaker_suffs, speaker_lower):\n",
    "            if p.startswith(s_suff):\n",
    "                speaker_lists[s_low].append(p.replace(s_suff, '').strip())\n",
    "                assigned = True\n",
    "                previous_speaker = s_low\n",
    "        if not assigned:\n",
    "            speaker_lists[previous_speaker].append(p)\n",
    "            speaker_lists['unlabeled'].append((previous_speaker, p))\n",
    "    return speaker_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# refactoring 1070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# IN PROGRESS - REPLACING ABOVE CELL 1 WITH FUNCTION CALLS\n",
    "url_1070 = r'https://erikamentari.wordpress.com/2018/02/27/jre-1070-jordan-peterson-transcript/'\n",
    "raw_1070 = get_corpus(url_1070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611\n",
      "163379\n"
     ]
    }
   ],
   "source": [
    "# IN PROGRESS - REPLACING BELOW CELL 2 WITH FUNCTION CALLS\n",
    "cleaned_1070 = clean_corpus(raw_1070, 8, 10, None, True, ('Joe Rogan: ', 'Joe: '), \n",
    "                            ('Dr Jordan B Peterson: ', 'Jordan: '))\n",
    "joined_1070 = ' '.join(cleaned_1070)\n",
    "\n",
    "print(len(cleaned_1070))\n",
    "print(len(joined_1070))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph count (Joe): \n",
      "306\n",
      "\n",
      "Paragraph count (Jordan): \n",
      "304\n",
      "\n",
      "Paragraph count (Unlabeled): \n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IN PROGRESS...\n",
    "p_1070 = separate_speakers(cleaned_1070, ['Joe', 'Jordan'])\n",
    "p_1070['joe'].insert(2, p_1070['unlabeled'].pop(0))\n",
    "for key, val in p_1070.items():\n",
    "    print('Paragraph count ({}): \\n{}\\n'.format(key.title(), len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('joe',\n",
       "  'Boom and we\\'re live. 12 Rules for Life. So without reading this…\"So, what you\\'re saying is…\"'),\n",
       " ('joe', '18:18')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_1070 = separate_speakers(cleaned_1070, ['Joe', 'Jordan'])\n",
    "text_1070['unlabeled']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Episode 1070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 611\n",
      "Words: 163379\n"
     ]
    }
   ],
   "source": [
    "# REPLACING\n",
    "url = r'https://erikamentari.wordpress.com/2018/02/27/jre-1070-jordan-peterson-transcript/'\n",
    "\n",
    "content = requests.get(url).text\n",
    "soup = BS(content, 'lxml')\n",
    "paragraphs = soup.find_all('p')\n",
    "paragraphs = [p.get_text().replace('Joe Rogan: ', 'Joe: ')\\\n",
    "              .replace('Dr Jordan B Peterson: ', \"Jordan: \")\\\n",
    "              for p in paragraphs[8:-10]]\n",
    "paragraph_text = ' '.join(paragraphs)\n",
    "\n",
    "print('Paragraphs:', len(paragraphs))\n",
    "print('Words:', len(paragraph_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogan paragraph count: 306\n",
      "Peterson paragraph count: 304\n",
      "Unassigned text: ['18:18']\n"
     ]
    }
   ],
   "source": [
    "# REPLACING\n",
    "rogan_p = [p.replace('Joe:', '').strip() for p in paragraphs if p.startswith('Joe:')]\n",
    "peterson_p = [p.replace('Jordan:', '').strip() for p in paragraphs if p.startswith('Jordan:')]\n",
    "unlabeled = [p for p in paragraphs if not p.startswith('Joe:') and not p.startswith('Jordan:')]\n",
    "rogan_p.insert(1, unlabeled.pop(0))\n",
    "\n",
    "print('Rogan paragraph count:', len(rogan_p))\n",
    "print('Peterson paragraph count:', len(peterson_p))\n",
    "print('Unassigned text:', unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count: 29119\n",
      "Rogan word count: 6736\n",
      "Peterson word count: 22383\n",
      "% of conversation (Rogan): 23.13%\n",
      "% of conversation (Peterson): 76.87%\n"
     ]
    }
   ],
   "source": [
    "rogan_words = ' '.join(rogan_p).split(' ')\n",
    "peterson_words = ' '.join(peterson_p).split(' ')\n",
    "rogan_count, peterson_count = len(rogan_words), len(peterson_words)\n",
    "total_count = rogan_count + peterson_count\n",
    "rogan_percent = rogan_count / total_count\n",
    "peterson_percent = peterson_count / total_count\n",
    "\n",
    "print('Total word count:', total_count)\n",
    "print('Rogan word count:', rogan_count)\n",
    "print('Peterson word count:', peterson_count)\n",
    "print('% of conversation (Rogan): {}%'.format(round(100* rogan_percent, 2)))\n",
    "print('% of conversation (Peterson): {}%'.format(round(100 * peterson_percent, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (Rogan)\n",
      "\n",
      "[('to', 187), ('of', 182), ('the', 178), ('I', 170), ('and', 157), ('a', 155), ('that', 145), ('you', 127), ('is', 126), ('this', 91), ('in', 87), ('people', 65), ('And', 64), ('what', 59), ('have', 59), ('it', 56), ('about', 53), ('think', 51), ('was', 50), ('are', 47), ('you’re', 42), ('not', 42), ('it’s', 41), ('with', 38), ('one', 35)]\n",
      "\n",
      "Most common words (Peterson)\n",
      "\n",
      "[('the', 868), ('to', 614), ('and', 524), ('you', 517), ('of', 498), ('a', 493), ('that', 466), ('I', 430), ('is', 291), ('in', 262), ('And', 244), ('it', 219), ('it’s', 195), ('like,', 189), ('that’s', 181), ('It’s', 174), ('what', 168), ('they', 156), ('have', 149), ('so', 144), ('was', 135), ('for', 134), ('people', 133), ('be', 128), ('do', 125)]\n"
     ]
    }
   ],
   "source": [
    "common_rogan = Counter(rogan_words).most_common()\n",
    "common_peterson = Counter(peterson_words).most_common()\n",
    "\n",
    "print('Most common words (Rogan)\\n\\n' + str(common_rogan[:25]))\n",
    "print('\\nMost common words (Peterson)\\n\\n' + str(common_peterson[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rogan_corpus = ' '.join(rogan_words)\n",
    "peterson_corpus = ' '.join(peterson_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogan words: 8000\n",
      "Peterson words: 27448\n"
     ]
    }
   ],
   "source": [
    "# replace curly quotes, then tokenize\n",
    "rogan_corpus, peterson_corpus = replace_curly_quotes(rogan_corpus, peterson_corpus)\n",
    "rogan_word_toke, rogan_p_toke = word_tokenize(rogan_corpus), sent_tokenize(rogan_corpus)\n",
    "peterson_word_toke, peterson_p_toke = word_tokenize(peterson_corpus), sent_tokenize(peterson_corpus)\n",
    "\n",
    "print('Rogan words: ' + str(len(rogan_word_toke)))\n",
    "print('Peterson words: ' + str(len(peterson_word_toke)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogan words (stop words removed): 4205\n",
      "Peterson words (stop words removed): 15093\n",
      "\n",
      "Rogan most common words:\n",
      "[('.', 449), (',', 368), (\"'s\", 148), ('people', 75), (\"'re\", 73), ('?', 62), ('think', 53), ('like', 45), (\"n't\", 45), ('one', 35), ('things', 30), (';', 25), (\"'m\", 24), ('Yeah', 24), ('Right', 23), ('know', 23), ('Yes', 22), ('get', 22), ('going', 22), ('mean', 21), ('way', 20), (\"'ve\", 20), ('saying', 19), ('right', 17), ('Well', 17)]\n",
      "\n",
      "Peterson most common words\n",
      "[(',', 1853), ('.', 1348), (\"'s\", 816), ('like', 308), (\"n't\", 230), (\"'re\", 200), ('?', 191), ('well', 165), (';', 163), ('people', 157), ('know', 150), ('think', 120), ('Yeah', 99), ('Well', 97), ('going', 94), (\"'m\", 84), ('right', 81), ('say', 80), ('things', 70), ('way', 65), ('want', 65), ('one', 61), ('life', 58), ('thing', 58), ('get', 58)]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "rogan_word_toke = [w for w in rogan_word_toke if w.lower() not in stop_words]\n",
    "peterson_word_toke = [w for w in peterson_word_toke if w.lower() not in stop_words]\n",
    "\n",
    "print('Rogan words (stop words removed): ' + str(len(rogan_word_toke)))\n",
    "print('Peterson words (stop words removed): ' + str(len(peterson_word_toke)))\n",
    "print('\\nRogan most common words:\\n' + str(Counter(rogan_word_toke).most_common(25)))\n",
    "print('\\nPeterson most common words\\n' + str(Counter(peterson_word_toke).most_common(25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rogan_tagged = nltk.pos_tag(rogan_word_toke)\n",
    "peterson_tagged = nltk.pos_tag(rogan_word_toke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk(tagged):\n",
    "    '''Pass in tagged word tokens and find chunks.'''\n",
    "    chunk_gram = '''chunk_name: {<JJ.?>+<NN.?>+}'''\n",
    "    chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "    chunked = chunk_parser.parse(tagged)\n",
    "    #chunked.draw()\n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy_widget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +\n\u001b[0m\u001b[1;32m    731\u001b[0m                             \u001b[1;34m'-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                             .format(out_path, in_path).split())\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[1;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 604\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 598\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         (filename, url))\n\u001b[1;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n==========================================================================="
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('guest', 'NN'), ('today', 'NN'), Tree('chunk_name', [('great', 'JJ'), ('powerful', 'JJ'), ('Jordan', 'NNP'), ('Peterson', 'NNP')]), ('.', '.'), ('Jordan', 'NNP'), ('podcast', 'NN'), ('multiple', 'NN'), ('times', 'NNS'), (',', ','), (\"'s\", 'VBZ'), ('one', 'CD'), Tree('chunk_name', [('favorite', 'JJ'), ('human', 'JJ'), ('beings', 'NNS'), ('talk', 'NN')]), (',', ','), (\"'m\", 'VBP'), Tree('chunk_name', [('happy', 'JJ'), ('exists', 'NNS')]), ('.', '.'), (\"'s\", 'POS'), Tree('chunk_name', [('brilliant', 'JJ'), ('man', 'NN')]), ('amazing', 'VBG'), ('book', 'NN'), ('right', 'NN'), (\"'s\", 'POS'), ('called', 'VBN'), ('twelve', 'NN'), ('rules', 'NNS'), ('life', 'NN'), (':', ':'), Tree('chunk_name', [('antidote', 'JJ'), ('chaos', 'NN')]), ('one', 'CD'), Tree('chunk_name', [('important', 'JJ'), ('messages', 'NNS')]), Tree('chunk_name', [('brilliant', 'JJ'), ('man', 'NN')]), (',', ','), ('please', 'VB'), Tree('chunk_name', [('welcome', 'JJ'), ('Jordan', 'NNP'), ('Peterson', 'NNP')]), ('.', '.'), ('Boom', 'NNP'), (\"'re\", 'VBP'), ('live', 'JJ'), ('.', '.'), ('12', 'CD'), ('Rules', 'NNPS'), ('Life', 'NNP'), ('.', '.'), ('without', 'IN'), ('reading', 'VBG'), ('this…', 'NN'), (\"''\", \"''\"), (',', ','), (\"'re\", 'VBP'), ('saying', 'VBG'), ('is…', 'NN'), (\"''\", \"''\"), (',', ','), Tree('chunk_name', [('um—', 'JJ'), ('interview', 'NN'), ('woman', 'NN'), ('Cathy', 'NNP'), ('Newman', 'NNP')]), (',', ','), ('uh', 'UH'), (',', ','), ('UK', 'NNP'), ('?', '.'), ('Um', 'NNP'), (',', ','), ('went—', 'VBD'), ('felt', 'NN'), ('bad', 'JJ'), ('also', 'RB'), ('laughing—', 'VBP'), ('went', 'VBD'), ('Twitter', 'NNP'), ('page', 'NN'), ('read', 'VBD'), (',', ','), ('like', 'IN'), (',', ','), ('one', 'CD'), ('tweets', 'NNS'), (',', ','), ('matter', 'NN'), ('says', 'VBZ'), (';', ':'), ('someone', 'NN'), ('writes', 'VBZ'), ('underneath', 'IN'), (',', ',')])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk(rogan_tagged[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import string\n",
    "\n",
    "# print(string.punctuation)\n",
    "# remove_punct = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "# a = 'abdlkja;c,3dk!.ejf}]dk'\n",
    "# print(a)\n",
    "# print(a.translate(remove_punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url2 = ('https://beyondhumannature.wordpress.com/2018/03/12/joe-rogan-experience'\n",
    "        '-ep-877-with-jordan-peterson-transcript/')\n",
    "raw_p2 = get_corpus(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text paragraphs: 423\n",
      "Cleaned text paragraphs: 389\n",
      "Rogan paragraphs: 178\n",
      "Peterson paragraphs: 210\n",
      "\n",
      "Unassigned paragraphs: [MUSIC 172:38]\n"
     ]
    }
   ],
   "source": [
    "p2 = clean_corpus(raw_p2, 11, 16, '\\xa0', True)\n",
    "texts = separate_speakers(p2, ['ROGAN', 'PETERSON'])\n",
    "texts['unlabeled'] = texts['unlabeled'][1]\n",
    "texts['rogan'].remove(texts['unlabeled'][1])\n",
    "\n",
    "print('Raw text paragraphs:', len(raw_p2))\n",
    "print('Cleaned text paragraphs:', len(p2))\n",
    "print('Rogan paragraphs:', len(texts['rogan']))\n",
    "print('Peterson paragraphs:', len(texts['peterson']))\n",
    "print('\\nUnassigned paragraphs:', texts['unlabeled'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "rogan_corpus_877 = ' '.join(texts['rogan'])\n",
    "cv_count_877 = cv.fit_transform([rogan_corpus_877])\n",
    "tfidf_count_877 = tfidf.fit_transform([rogan_corpus_877])\n",
    "\n",
    "rogan_words_877 = np.array(cv.get_feature_names())\n",
    "rogan_counts_877 = cv_count_877.toarray().flatten()\n",
    "rogan_tfidf_877 = tfidf_count_877.toarray().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scaled_count</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>0.458033</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.286270</td>\n",
       "      <td>just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>0.278091</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0.253554</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>0.171762</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>0.171762</td>\n",
       "      <td>ve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>0.147225</td>\n",
       "      <td>don</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>0.147225</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>0.139046</td>\n",
       "      <td>going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>0.139046</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>0.122687</td>\n",
       "      <td>really</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.114508</td>\n",
       "      <td>things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>0.106329</td>\n",
       "      <td>university</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>0.106329</td>\n",
       "      <td>woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>0.106329</td>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.106329</td>\n",
       "      <td>believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>0.098150</td>\n",
       "      <td>pronouns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>0.098150</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>0.089971</td>\n",
       "      <td>mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.089971</td>\n",
       "      <td>did</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0.089971</td>\n",
       "      <td>sort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>0.081792</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.081792</td>\n",
       "      <td>use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>0.081792</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>0.081792</td>\n",
       "      <td>problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>0.081792</td>\n",
       "      <td>social</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>0.073612</td>\n",
       "      <td>videos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.073612</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.065433</td>\n",
       "      <td>doing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>0.065433</td>\n",
       "      <td>guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>lines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>listening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>lock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>longer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>chooses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>loose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>categories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>lover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>making</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>letters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>chambers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>legal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>leave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>lean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>laughing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>language</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>landlords</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>landlord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>kinds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>kinder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>channel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>jump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.008179</td>\n",
       "      <td>1930s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>826 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     scaled_count        word\n",
       "521      0.458033      people\n",
       "400      0.286270        just\n",
       "721      0.278091       think\n",
       "424      0.253554        like\n",
       "620      0.171762       right\n",
       "773      0.171762          ve\n",
       "210      0.147225         don\n",
       "794      0.147225         way\n",
       "309      0.139046       going\n",
       "630      0.139046         say\n",
       "593      0.122687      really\n",
       "720      0.114508      things\n",
       "762      0.106329  university\n",
       "801      0.106329       woman\n",
       "818      0.106329        yeah\n",
       "60       0.106329     believe\n",
       "573      0.098150    pronouns\n",
       "821      0.098150         yes\n",
       "459      0.089971        mean\n",
       "182      0.089971         did\n",
       "662      0.089971        sort\n",
       "447      0.081792         man\n",
       "767      0.081792         use\n",
       "438      0.081792         lot\n",
       "559      0.081792     problem\n",
       "657      0.081792      social\n",
       "776      0.073612      videos\n",
       "407      0.073612        know\n",
       "207      0.065433       doing\n",
       "323      0.065433         guy\n",
       "..            ...         ...\n",
       "425      0.008179       lines\n",
       "427      0.008179   listening\n",
       "99       0.008179      center\n",
       "432      0.008179        lock\n",
       "434      0.008179      longer\n",
       "108      0.008179     chooses\n",
       "97       0.008179    category\n",
       "437      0.008179       loose\n",
       "96       0.008179  categories\n",
       "440      0.008179       lover\n",
       "445      0.008179      making\n",
       "92       0.008179        care\n",
       "422      0.008179     letters\n",
       "102      0.008179    chambers\n",
       "419      0.008179       legal\n",
       "417      0.008179       leave\n",
       "416      0.008179       learn\n",
       "415      0.008179        lean\n",
       "1        0.008179          70\n",
       "411      0.008179    laughing\n",
       "410      0.008179    language\n",
       "409      0.008179   landlords\n",
       "408      0.008179    landlord\n",
       "104      0.008179      change\n",
       "406      0.008179        king\n",
       "405      0.008179       kinds\n",
       "404      0.008179      kinder\n",
       "106      0.008179     channel\n",
       "399      0.008179        jump\n",
       "0        0.008179       1930s\n",
       "\n",
       "[826 rows x 2 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf2 = TfidfVectorizer(stop_words='english')\n",
    "df_877_stop = pd.DataFrame(\n",
    "    {'scaled_count': tfidf2.fit_transform([rogan_corpus_877]).toarray().flatten(),\n",
    "    'word': np.array(tfidf2.get_feature_names())})\n",
    "df_877_stop.sort_values('scaled_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>scaled_count</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>180</td>\n",
       "      <td>0.375681</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>177</td>\n",
       "      <td>0.369419</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>165</td>\n",
       "      <td>0.344374</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>152</td>\n",
       "      <td>0.317241</td>\n",
       "      <td>you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>136</td>\n",
       "      <td>0.283848</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>119</td>\n",
       "      <td>0.248367</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>101</td>\n",
       "      <td>0.210799</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>98</td>\n",
       "      <td>0.204537</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>79</td>\n",
       "      <td>0.164882</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>71</td>\n",
       "      <td>0.148185</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>65</td>\n",
       "      <td>0.135662</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>60</td>\n",
       "      <td>0.125227</td>\n",
       "      <td>what</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>59</td>\n",
       "      <td>0.123140</td>\n",
       "      <td>re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>56</td>\n",
       "      <td>0.116878</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>45</td>\n",
       "      <td>0.093920</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>39</td>\n",
       "      <td>0.081397</td>\n",
       "      <td>so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>38</td>\n",
       "      <td>0.079310</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>37</td>\n",
       "      <td>0.077223</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>37</td>\n",
       "      <td>0.077223</td>\n",
       "      <td>are</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>37</td>\n",
       "      <td>0.077223</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>36</td>\n",
       "      <td>0.075136</td>\n",
       "      <td>these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>35</td>\n",
       "      <td>0.073049</td>\n",
       "      <td>just</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>34</td>\n",
       "      <td>0.070962</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>33</td>\n",
       "      <td>0.068875</td>\n",
       "      <td>there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>31</td>\n",
       "      <td>0.064701</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>30</td>\n",
       "      <td>0.062613</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>28</td>\n",
       "      <td>0.058439</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>27</td>\n",
       "      <td>0.056352</td>\n",
       "      <td>them</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>27</td>\n",
       "      <td>0.056352</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>0.054265</td>\n",
       "      <td>about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>pointing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>edged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>educate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>plural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>educated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>pick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>egalitarian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>either</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>perpetually</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>elf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>elite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>peers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>pay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>environments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>endlessly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>patriarchy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>past</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>particularly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>enforce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>pancho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>pages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>overcharging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>enter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>outlined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>environmental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>1930s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     count  scaled_count           word\n",
       "846    180      0.375681            the\n",
       "39     177      0.369419            and\n",
       "845    165      0.344374           that\n",
       "992    152      0.317241            you\n",
       "583    136      0.283848             of\n",
       "870    119      0.248367             to\n",
       "469    101      0.210799             it\n",
       "465     98      0.204537             is\n",
       "860     79      0.164882           this\n",
       "853     71      0.148185           they\n",
       "433     65      0.135662             in\n",
       "954     60      0.125227           what\n",
       "700     59      0.123140             re\n",
       "627     56      0.116878         people\n",
       "968     45      0.093920           with\n",
       "775     39      0.081397             so\n",
       "387     38      0.079310           have\n",
       "576     37      0.077223            not\n",
       "48      37      0.077223            are\n",
       "591     37      0.077223            one\n",
       "852     36      0.075136          these\n",
       "475     35      0.073049           just\n",
       "856     34      0.070962          think\n",
       "851     33      0.068875          there\n",
       "502     31      0.064701           like\n",
       "943     30      0.062613            was\n",
       "327     28      0.058439            for\n",
       "848     27      0.056352           them\n",
       "589     27      0.056352             on\n",
       "6       26      0.054265          about\n",
       "..     ...           ...            ...\n",
       "641      1      0.002087       pointing\n",
       "255      1      0.002087          edged\n",
       "256      1      0.002087        educate\n",
       "637      1      0.002087         plural\n",
       "636      1      0.002087         please\n",
       "257      1      0.002087       educated\n",
       "633      1      0.002087           pick\n",
       "260      1      0.002087    egalitarian\n",
       "261      1      0.002087         either\n",
       "629      1      0.002087    perpetually\n",
       "262      1      0.002087            elf\n",
       "263      1      0.002087          elite\n",
       "626      1      0.002087          peers\n",
       "624      1      0.002087            pay\n",
       "275      1      0.002087   environments\n",
       "265      1      0.002087      endlessly\n",
       "622      1      0.002087        pattern\n",
       "621      1      0.002087     patriarchy\n",
       "620      1      0.002087           past\n",
       "619      1      0.002087         passed\n",
       "618      1      0.002087   particularly\n",
       "266      1      0.002087        enforce\n",
       "616      1      0.002087         pancho\n",
       "615      1      0.002087          pages\n",
       "271      1      0.002087         enough\n",
       "612      1      0.002087   overcharging\n",
       "272      1      0.002087          enter\n",
       "609      1      0.002087       outlined\n",
       "274      1      0.002087  environmental\n",
       "0        1      0.002087          1930s\n",
       "\n",
       "[999 rows x 3 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_877 = pd.DataFrame({'word': rogan_words_877, 'count': rogan_counts_877,\n",
    "                       'scaled_count': rogan_tfidf_877})\n",
    "df_877.sort_values('scaled_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
