{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tag list\n",
    "\n",
    "**CC**\tcoordinating conjunction<br>\n",
    "**CD**\tcardinal digit<br>\n",
    "**DT**\tdeterminer<br>\n",
    "**EX**\texistential there (like: \"there is\" ... think of it like \"there exists\")<br>\n",
    "**FW**\tforeign word<br>\n",
    "**IN**\tpreposition/subordinating conjunction<br>\n",
    "**JJ**\tadjective\t'big'<br>\n",
    "**JJR**\tadjective, comparative\t'bigger'<br>\n",
    "**JJS**\tadjective, superlative\t'biggest'<br>\n",
    "**LS**\tlist marker\t1)<br>\n",
    "**MD**\tmodal\tcould, will<br>\n",
    "**NN**\tnoun, singular 'desk'<br>\n",
    "**NNS**\tnoun plural\t'desks'<br>\n",
    "**NNP**\tproper noun, singular\t'Harrison'<br>\n",
    "**NNPS**\tproper noun, plural\t'Americans'<br>\n",
    "**PDT**\tpredeterminer\t'all the kids'<br>\n",
    "**POS**\tpossessive ending\tparent's<br>\n",
    "**PRP**\tpersonal pronoun\tI, he, she<br>\n",
    "**PRP\\$**\tpossessive pronoun\tmy, his, hers<br>\n",
    "**RB**\tadverb\tvery, silently,<br>\n",
    "**RBR**\tadverb, comparative\tbetter<br>\n",
    "**RBS**\tadverb, superlative\tbest<br>\n",
    "**RP**\tparticle\tgive up<br>\n",
    "**TO**\tto\tgo 'to' the store.<br>\n",
    "**UH**\tinterjection\terrrrrrrrm<br>\n",
    "**VB**\tverb, base form\ttake<br>\n",
    "**VBD**\tverb, past tense\ttook<br>\n",
    "**VBG**\tverb, gerund/present participle\ttaking<br>\n",
    "**VBN**\tverb, past participle\ttaken<br>\n",
    "**VBP**\tverb, sing. present, non-3d\ttake<br>\n",
    "**VBZ**\tverb, 3rd person sing. present\ttakes<br>\n",
    "**WDT**\twh-determiner\twhich<br>\n",
    "**WP**\twh-pronoun\twho, what<br>\n",
    "**WP\\$**\tpossessive wh-pronoun\twhose<br>\n",
    "**WRB**\twh-abverb\twhere, when<br>\n",
    "\n",
    "### Stopwords\n",
    "\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",<br>\n",
    "\"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', <br>\n",
    "'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\",<br>\n",
    "'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', <br>\n",
    "'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', <br>\n",
    "'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', <br>\n",
    "'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',<br>\n",
    "'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about',<br>\n",
    "'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above',<br>\n",
    "'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under',<br>\n",
    "'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why',<br>\n",
    "'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',<br>\n",
    "'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',<br>\n",
    "'s', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now',<br>\n",
    "'d', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",<br>\n",
    "'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', <br>\n",
    "\"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn',<br>\n",
    "\"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', <br>\n",
    "\"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_corpus(url):\n",
    "    '''Pass in url. Retrieve content and return list of paragraphs.'''\n",
    "    content = requests.get(url).text\n",
    "    soup = BS(content, 'lxml')\n",
    "    paragraphs = soup.find_all('p')\n",
    "    paragraphs = [p.get_text() for p in paragraphs]\n",
    "    corpus = ' '.join(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def view_tails(paragraphs, start=10, end=10):\n",
    "    '''Pass in list of paragraphs and print out first 5 and last 5.\n",
    "    Use to quickly check what needs to be trimmed.'''\n",
    "    pprint(paragraphs[:start])\n",
    "    print('\\n')\n",
    "    pprint(paragraphs[-end:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_corpus(paragraphs, trim_start=0, trim_end=0, delete=None, *args):\n",
    "    '''Pass in list of paragraphs, # of paragraphs to drop from beginning\n",
    "    and end, and tuples of format (to_replace, replace_with).'''\n",
    "    trim_end = len(paragraphs) - trim_end\n",
    "    if delete is not None:\n",
    "        paragraphs = [p for p in paragraphs if p != delete]\n",
    "    paragraphs = paragraphs[trim_start:trim_end]\n",
    "    for arg in args:\n",
    "        paragraphs = [p.replace(arg[0], arg[1]) for p in paragraphs]\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def separate_speakers(paragraphs, suffix=': ', *args):\n",
    "    '''Pass in list of paragraphs and speaker names as strings. Return list \n",
    "    of paragraphs for each speaker.'''\n",
    "    speaker_paragraphs = {}\n",
    "    for arg in args:\n",
    "        arg_suff = arg + suffix\n",
    "        speaker_paragraphs[arg.lower()] = [p for p in paragraphs if p.startswith(arg_suff)]\n",
    "    speaker_paragraphs['unlabeled'] = [p for p in paragraphs \n",
    "                       if not any(p in ls for ls in speaker_paragraphs.values())]\n",
    "    return speaker_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_curly_quotes(*corpora):\n",
    "    '''Pass in 1 or more corpora and replace all curly quotes\n",
    "    with standard quotes.\n",
    "    '''\n",
    "    output = []\n",
    "    for i, corpus in enumerate(corpora):\n",
    "        new_corpus = corpus.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\")\n",
    "        output.append(new_corpus)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b': ['abc', 'ghi', 'mno', 'stu', 'yz'], 'c': ['abc', 'def', 'ghi', 'vwx', 'yz']}\n",
      "['jkl', 'pqr']\n",
      "dict_values([['abc', 'ghi', 'mno', 'stu', 'yz'], ['abc', 'def', 'ghi', 'vwx', 'yz']])\n",
      "5\n",
      "test\n",
      "steven\n",
      "major\n"
     ]
    }
   ],
   "source": [
    "a = ['abc', 'def', 'ghi', 'jkl', 'mno', 'pqr', 'stu', 'vwx', 'yz']\n",
    "b = [group for i, group in enumerate(a) if i % 2 == 0]\n",
    "c = [group for i, group in enumerate(a) if i < 3 or i > 6]\n",
    "a_dict = {'b': b,\n",
    "         'c': c}\n",
    "print(a_dict)\n",
    "d = [group for group in a if not any(group in val for val in a_dict.values())]\n",
    "print(d)\n",
    "\n",
    "print(a_dict.values())\n",
    "\n",
    "def f_args(name, age=3, **kwargs):\n",
    "    for key, val in kwargs.items():\n",
    "        print(val)\n",
    "        print(key)\n",
    "f_args(name='jason', age=4, test=5, major='steven')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Episode 1070"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraphs: 611\n",
      "Words: 163379\n"
     ]
    }
   ],
   "source": [
    "url = r'https://erikamentari.wordpress.com/2018/02/27/jre-1070-jordan-peterson-transcript/'\n",
    "\n",
    "content = requests.get(url).text\n",
    "soup = BS(content, 'lxml')\n",
    "paragraphs = soup.find_all('p')\n",
    "paragraphs = [p.get_text().replace('Joe Rogan: ', 'Joe: ')\\\n",
    "              .replace('Dr Jordan B Peterson: ', \"Jordan: \")\\\n",
    "              for p in paragraphs[8:-10]]\n",
    "paragraph_text = ' '.join(paragraphs)\n",
    "\n",
    "print('Paragraphs:', len(paragraphs))\n",
    "print('Words:', len(paragraph_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611\n",
      "163379\n",
      "Joe 303\n",
      "Jordan 304\n",
      "unlabeled 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IN PROGRESS - REPLACING ABOVE CELL WITH NEW FUNCTION CALLS\n",
    "test_p = get_corpus(url)\n",
    "cleaned_test = clean_corpus(test_p, 8, 10, None, ('Joe Rogan: ', 'Joe: '), \n",
    "                            ('Dr Jordan B Peterson: ', 'Jordan: '))\n",
    "test_text = ' '.join(cleaned_test)\n",
    "\n",
    "print(len(cleaned_test))\n",
    "print(len(test_text))\n",
    "\n",
    "test_dict = separate_speakers(cleaned_test, ': ', 'Joe', 'Jordan')\n",
    "for key, val in test_dict.items():\n",
    "    print(key, len(val))\n",
    "cleaned_test == test_dict['unlabeled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogan paragraph count: 306\n",
      "Peterson paragraph count: 304\n",
      "Unassigned text: ['18:18']\n"
     ]
    }
   ],
   "source": [
    "rogan_p = [p.replace('Joe:', '').strip() for p in paragraphs if p.startswith('Joe:')]\n",
    "peterson_p = [p.replace('Jordan:', '').strip() for p in paragraphs if p.startswith('Jordan:')]\n",
    "unlabeled = [p for p in paragraphs if not p.startswith('Joe:') and not p.startswith('Jordan:')]\n",
    "rogan_p.insert(1, unlabeled.pop(0))\n",
    "\n",
    "print('Rogan paragraph count:', len(rogan_p))\n",
    "print('Peterson paragraph count:', len(peterson_p))\n",
    "print('Unassigned text:', unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word count: 29119\n",
      "Rogan word count: 6736\n",
      "Peterson word count: 22383\n",
      "% of conversation (Rogan): 23.13%\n",
      "% of conversation (Peterson): 76.87%\n"
     ]
    }
   ],
   "source": [
    "rogan_words = ' '.join(rogan_p).split(' ')\n",
    "peterson_words = ' '.join(peterson_p).split(' ')\n",
    "rogan_count, peterson_count = len(rogan_words), len(peterson_words)\n",
    "total_count = rogan_count + peterson_count\n",
    "rogan_percent = rogan_count / total_count\n",
    "peterson_percent = peterson_count / total_count\n",
    "\n",
    "print('Total word count:', total_count)\n",
    "print('Rogan word count:', rogan_count)\n",
    "print('Peterson word count:', peterson_count)\n",
    "print('% of conversation (Rogan): {}%'.format(round(100* rogan_percent, 2)))\n",
    "print('% of conversation (Peterson): {}%'.format(round(100 * peterson_percent, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (Rogan)\n",
      "\n",
      "[('to', 187), ('of', 182), ('the', 178), ('I', 170), ('and', 157), ('a', 155), ('that', 145), ('you', 127), ('is', 126), ('this', 91), ('in', 87), ('people', 65), ('And', 64), ('what', 59), ('have', 59), ('it', 56), ('about', 53), ('think', 51), ('was', 50), ('are', 47), ('you’re', 42), ('not', 42), ('it’s', 41), ('with', 38), ('one', 35)]\n",
      "\n",
      "Most common words (Peterson)\n",
      "\n",
      "[('the', 868), ('to', 614), ('and', 524), ('you', 517), ('of', 498), ('a', 493), ('that', 466), ('I', 430), ('is', 291), ('in', 262), ('And', 244), ('it', 219), ('it’s', 195), ('like,', 189), ('that’s', 181), ('It’s', 174), ('what', 168), ('they', 156), ('have', 149), ('so', 144), ('was', 135), ('for', 134), ('people', 133), ('be', 128), ('do', 125)]\n"
     ]
    }
   ],
   "source": [
    "common_rogan = Counter(rogan_words).most_common()\n",
    "common_peterson = Counter(peterson_words).most_common()\n",
    "\n",
    "print('Most common words (Rogan)\\n\\n' + str(common_rogan[:25]))\n",
    "print('\\nMost common words (Peterson)\\n\\n' + str(common_peterson[:25]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rogan_corpus = ' '.join(rogan_words)\n",
    "peterson_corpus = ' '.join(peterson_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogan words: 8000\n",
      "Peterson words: 27448\n"
     ]
    }
   ],
   "source": [
    "# replace curly quotes, then tokenize\n",
    "rogan_corpus, peterson_corpus = replace_curly_quotes(rogan_corpus, peterson_corpus)\n",
    "rogan_word_toke, rogan_p_toke = word_tokenize(rogan_corpus), sent_tokenize(rogan_corpus)\n",
    "peterson_word_toke, peterson_p_toke = word_tokenize(peterson_corpus), sent_tokenize(peterson_corpus)\n",
    "\n",
    "print('Rogan words: ' + str(len(rogan_word_toke)))\n",
    "print('Peterson words: ' + str(len(peterson_word_toke)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogan words (stop words removed): 4205\n",
      "Peterson words (stop words removed): 15093\n",
      "\n",
      "Rogan most common words:\n",
      "[('.', 449), (',', 368), (\"'s\", 148), ('people', 75), (\"'re\", 73), ('?', 62), ('think', 53), ('like', 45), (\"n't\", 45), ('one', 35), ('things', 30), (';', 25), (\"'m\", 24), ('Yeah', 24), ('Right', 23), ('know', 23), ('Yes', 22), ('get', 22), ('going', 22), ('mean', 21), ('way', 20), (\"'ve\", 20), ('saying', 19), ('right', 17), ('Well', 17)]\n",
      "\n",
      "Peterson most common words\n",
      "[(',', 1853), ('.', 1348), (\"'s\", 816), ('like', 308), (\"n't\", 230), (\"'re\", 200), ('?', 191), ('well', 165), (';', 163), ('people', 157), ('know', 150), ('think', 120), ('Yeah', 99), ('Well', 97), ('going', 94), (\"'m\", 84), ('right', 81), ('say', 80), ('things', 70), ('way', 65), ('want', 65), ('one', 61), ('life', 58), ('thing', 58), ('get', 58)]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "rogan_word_toke = [w for w in rogan_word_toke if w.lower() not in stop_words]\n",
    "peterson_word_toke = [w for w in peterson_word_toke if w.lower() not in stop_words]\n",
    "\n",
    "print('Rogan words (stop words removed): ' + str(len(rogan_word_toke)))\n",
    "print('Peterson words (stop words removed): ' + str(len(peterson_word_toke)))\n",
    "print('\\nRogan most common words:\\n' + str(Counter(rogan_word_toke).most_common(25)))\n",
    "print('\\nPeterson most common words\\n' + str(Counter(peterson_word_toke).most_common(25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rogan_tagged = nltk.pos_tag(rogan_word_toke)\n",
    "peterson_tagged = nltk.pos_tag(rogan_word_toke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chunk(tagged):\n",
    "    '''Pass in tagged word tokens and find chunks.'''\n",
    "    chunk_gram = '''chunk_name: {<JJ.?>+<NN.?>+}'''\n",
    "    chunk_parser = nltk.RegexpParser(chunk_gram)\n",
    "    chunked = chunk_parser.parse(tagged)\n",
    "    #chunked.draw()\n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy_widget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m             subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +\n\u001b[0m\u001b[1;32m    731\u001b[0m                             \u001b[1;34m'-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                             .format(out_path, in_path).split())\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[1;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 604\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    596\u001b[0m     \"\"\"\n\u001b[1;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 598\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\hmamin\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    567\u001b[0m                         (filename, url))\n\u001b[1;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n==========================================================================="
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('guest', 'NN'), ('today', 'NN'), Tree('chunk_name', [('great', 'JJ'), ('powerful', 'JJ'), ('Jordan', 'NNP'), ('Peterson', 'NNP')]), ('.', '.'), ('Jordan', 'NNP'), ('podcast', 'NN'), ('multiple', 'NN'), ('times', 'NNS'), (',', ','), (\"'s\", 'VBZ'), ('one', 'CD'), Tree('chunk_name', [('favorite', 'JJ'), ('human', 'JJ'), ('beings', 'NNS'), ('talk', 'NN')]), (',', ','), (\"'m\", 'VBP'), Tree('chunk_name', [('happy', 'JJ'), ('exists', 'NNS')]), ('.', '.'), (\"'s\", 'POS'), Tree('chunk_name', [('brilliant', 'JJ'), ('man', 'NN')]), ('amazing', 'VBG'), ('book', 'NN'), ('right', 'NN'), (\"'s\", 'POS'), ('called', 'VBN'), ('twelve', 'NN'), ('rules', 'NNS'), ('life', 'NN'), (':', ':'), Tree('chunk_name', [('antidote', 'JJ'), ('chaos', 'NN')]), ('one', 'CD'), Tree('chunk_name', [('important', 'JJ'), ('messages', 'NNS')]), Tree('chunk_name', [('brilliant', 'JJ'), ('man', 'NN')]), (',', ','), ('please', 'VB'), Tree('chunk_name', [('welcome', 'JJ'), ('Jordan', 'NNP'), ('Peterson', 'NNP')]), ('.', '.'), ('Boom', 'NNP'), (\"'re\", 'VBP'), ('live', 'JJ'), ('.', '.'), ('12', 'CD'), ('Rules', 'NNPS'), ('Life', 'NNP'), ('.', '.'), ('without', 'IN'), ('reading', 'VBG'), ('this…', 'NN'), (\"''\", \"''\"), (',', ','), (\"'re\", 'VBP'), ('saying', 'VBG'), ('is…', 'NN'), (\"''\", \"''\"), (',', ','), Tree('chunk_name', [('um—', 'JJ'), ('interview', 'NN'), ('woman', 'NN'), ('Cathy', 'NNP'), ('Newman', 'NNP')]), (',', ','), ('uh', 'UH'), (',', ','), ('UK', 'NNP'), ('?', '.'), ('Um', 'NNP'), (',', ','), ('went—', 'VBD'), ('felt', 'NN'), ('bad', 'JJ'), ('also', 'RB'), ('laughing—', 'VBP'), ('went', 'VBD'), ('Twitter', 'NNP'), ('page', 'NN'), ('read', 'VBD'), (',', ','), ('like', 'IN'), (',', ','), ('one', 'CD'), ('tweets', 'NNS'), (',', ','), ('matter', 'NN'), ('says', 'VBZ'), (';', ':'), ('someone', 'NN'), ('writes', 'VBZ'), ('underneath', 'IN'), (',', ',')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk(rogan_tagged[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "abdlkja;c,3dk!.ejf}]dk\n",
      "abdlkja c 3dk  ejf  dk\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)\n",
    "remove_punct = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "a = 'abdlkja;c,3dk!.ejf}]dk'\n",
    "print(a)\n",
    "#b = 'abdlkja;c,3dk!.ejf}]dk'.remove(string.punctuation)\n",
    "print(a.translate(remove_punct))\n",
    "#print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jordan Peterson, Podcast, Psychology']\n",
      "\n",
      "\n",
      "['\\xa0',\n",
      " 'PETERSON: I’ve had lots, I’ve had lots of letters, obviously. Maybe, I don’t '\n",
      " 'know, 2500 letters maybe to my email accounts now about this, and a very '\n",
      " 'large number of them—maybe two hundred letters, a hundred and fifty to two '\n",
      " 'hundred—have been from people on the radical left who’ve written to me and '\n",
      " 'said that they can no longer speak because the authoritarian types, the PC '\n",
      " 'authoritarians, have got so controlling that their once fashionable position '\n",
      " 'is now being deemed unacceptable, and they’re alienated and excluded. When '\n",
      " 'you see that happening with feminists like Germaine Greer, I mean—Germaine '\n",
      " 'Greer who’s been banned from campuses—she’s not very happy with the idea '\n",
      " 'that being a woman is something that’s been reduced to a whim, right, '\n",
      " 'because she thinks that there’s more to being a woman than mere subjective '\n",
      " 'choice. Well, that’s no longer a tenemal viewpoint on the left, and so '\n",
      " 'people who hold that viewpoint, and many of them are feminists, are no '\n",
      " 'longer getting along with the, say, radical gender-bender-activist types '\n",
      " 'who’ve got centre stage at the moment, so.',\n",
      " '79:35',\n",
      " '—',\n",
      " '\\xa0',\n",
      " '\\n\\n\\t\\t\\t\\tView all posts by janearvine\\t\\t\\t\\n',\n",
      " 'Fill in your details below or click an icon to log in:',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\t\\t\\tYou are commenting using your WordPress.com account.\\t\\t\\t\\n'\n",
      " '\\t\\t\\t\\t(\\xa0Log\\xa0Out\\xa0/\\xa0\\n'\n",
      " '\\t\\t\\t\\tChange\\xa0)\\n'\n",
      " '\\t\\t\\t\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\t\\t\\tYou are commenting using your Google+ account.\\t\\t\\t\\n'\n",
      " '\\t\\t\\t\\t(\\xa0Log\\xa0Out\\xa0/\\xa0\\n'\n",
      " '\\t\\t\\t\\tChange\\xa0)\\n'\n",
      " '\\t\\t\\t\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\t\\t\\tYou are commenting using your Twitter account.\\t\\t\\t\\n'\n",
      " '\\t\\t\\t\\t(\\xa0Log\\xa0Out\\xa0/\\xa0\\n'\n",
      " '\\t\\t\\t\\tChange\\xa0)\\n'\n",
      " '\\t\\t\\t\\n'\n",
      " '\\n',\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\t\\t\\tYou are commenting using your Facebook account.\\t\\t\\t\\n'\n",
      " '\\t\\t\\t\\t(\\xa0Log\\xa0Out\\xa0/\\xa0\\n'\n",
      " '\\t\\t\\t\\tChange\\xa0)\\n'\n",
      " '\\t\\t\\t\\n'\n",
      " '\\n',\n",
      " 'Connecting to %s',\n",
      " ' Notify me of new comments via email.',\n",
      " ' \\n\\n',\n",
      " '',\n",
      " '',\n",
      " '© 2018 Beyond human nature',\n",
      " 'Blog at WordPress.com.']\n",
      "423\n",
      "396\n",
      "174\n",
      "174\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "url2 = ('https://beyondhumannature.wordpress.com/2018/03/12/joe-rogan-experience'\n",
    "        '-ep-877-with-jordan-peterson-transcript/')\n",
    "raw_p2 = get_corpus(url2)\n",
    "view_tails(raw_p2, 1, 18)\n",
    "p2 = clean_corpus(raw_p2, 11, 16, '\\xa0')\n",
    "texts = separate_speakers(p2, ': ', 'ROGAN', 'PETERSON')\n",
    "\n",
    "print(len(raw_p2))\n",
    "print(len(p2))\n",
    "print(len(texts['rogan']))\n",
    "print(len(texts['peterson']))\n",
    "print(len(texts['unlabeled']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 —\n",
      "1 \n",
      "\n",
      "\t\t\t\tView all posts by janearvine\t\t\t\n",
      "\n",
      "2 Fill in your details below or click an icon to log in:\n",
      "3 \n",
      "\n",
      "\t\t\tYou are commenting using your WordPress.com account.\t\t\t\n",
      "\t\t\t\t( Log Out / \n",
      "\t\t\t\tChange )\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "4 \n",
      "\n",
      "\t\t\tYou are commenting using your Google+ account.\t\t\t\n",
      "\t\t\t\t( Log Out / \n",
      "\t\t\t\tChange )\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "5 \n",
      "\n",
      "\t\t\tYou are commenting using your Twitter account.\t\t\t\n",
      "\t\t\t\t( Log Out / \n",
      "\t\t\t\tChange )\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "6 \n",
      "\n",
      "\t\t\tYou are commenting using your Facebook account.\t\t\t\n",
      "\t\t\t\t( Log Out / \n",
      "\t\t\t\tChange )\n",
      "\t\t\t\n",
      "\n",
      "\n",
      "7 Connecting to %s\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(texts['unlabeled'][-8:]):\n",
    "    print(i, text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
